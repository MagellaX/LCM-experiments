{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN5Alb3COOVSfaz9uRucv0G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Base-LCM Architecture Components\n","class PreNet(nn.Module):\n","    \"\"\"\n","    # Maps the input embeddings to the model's 'hidden dimension' after normalization...\n","\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim):\n","        super(PreNet, self).__init__()\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","        self.scaler_mean = 0.0  # Placeholder for robust scaler mean\n","        self.scaler_std = 1.0   # Placeholder for robust scaler std\n","\n","    def normalize(self, x):\n","        return (x - self.scaler_mean) / self.scaler_std\n","\n","    def forward(self, x):\n","        x = self.normalize(x)\n","        x = self.linear(x)\n","        return x\n","\n","class PostNet(nn.Module):\n","    \"\"\"\n","    Maps hidden state outputs back to the embedding space with denormalization.\n","    \"\"\"\n","    def __init__(self, hidden_dim, output_dim):\n","        super(PostNet, self).__init__()\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","        self.scaler_mean = 0.0  # Placeholder for robust scaler mean(means no effective scaling.)\n","        self.scaler_std = 1.0   # Placeholder for robust scaler std(means no effective scaling.)\n","\n","    def denormalize(self, x):\n","        return x * self.scaler_std + self.scaler_mean\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.denormalize(x)\n","        return x\n","\n","class TransformerDecoder(nn.Module):\n","    \"\"\"\n","    Standard Decoder-Only Transformer.\n","    \"\"\"\n","    def __init__(self, hidden_dim, num_heads, num_layers, ff_dim, dropout=0.1):\n","        super(TransformerDecoder, self).__init__()\n","        self.layers = nn.ModuleList([\n","            nn.TransformerDecoderLayer(\n","                d_model=hidden_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout\n","            )\n","            for _ in range(num_layers)\n","        ])\n","        self.pos_encoder = nn.Parameter(torch.zeros(1, 512, hidden_dim))  # Positional encoding\n","\n","    def forward(self, x):\n","        seq_len = x.size(1)\n","        x = x + self.pos_encoder[:, :seq_len]\n","        for layer in self.layers:\n","            x = layer(x, x)  # Self-attention in decoder layers\n","        return x\n","\n","class BaseLCM(nn.Module):\n","    \"\"\"\n","    Base Large Concept Model (LCM):\n","    - PreNet: Maps input embeddings to hidden space.\n","    - TransformerDecoder: Autoregressively processes embeddings.\n","    - PostNet: Maps output back to the embedding space.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim):\n","        super(BaseLCM, self).__init__()\n","        self.prenet = PreNet(input_dim, hidden_dim)\n","        self.transformer_decoder = TransformerDecoder(hidden_dim, num_heads, num_layers, ff_dim)\n","        self.postnet = PostNet(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.prenet(x)\n","        x = self.transformer_decoder(x)\n","        x = self.postnet(x)\n","        return x\n","\n","# Testing the Base-LCM architecture\n","def test_base_lcm():\n","    batch_size = 4\n","    sequence_length = 10\n","    input_dim = 256  # SONAR embedding dimension (e.g., pre-encoded sentences)\n","    hidden_dim = 512\n","    num_heads = 8\n","    num_layers = 6\n","    ff_dim = 2048\n","    output_dim = 256  # Output embedding dimension (same as input)\n","\n","    # Random input to simulate SONAR embeddings\n","    input_embeddings = torch.randn(batch_size, sequence_length, input_dim)\n","\n","    # Initialize and test Base-LCM\n","    model = BaseLCM(input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim)\n","    output_embeddings = model(input_embeddings)\n","\n","    print(\"Input shape:\", input_embeddings.shape)\n","    print(\"Output shape:\", output_embeddings.shape)\n","\n","if __name__ == \"__main__\":\n","    test_base_lcm()"],"metadata":{"id":"tnhRxaZyKaZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735123630835,"user_tz":-330,"elapsed":4131,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"4dc098d3-9fb0-4f15-cf0b-d079c08e92b0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([4, 10, 256])\n","Output shape: torch.Size([4, 10, 256])\n"]}]},{"cell_type":"code","source":["!pip install geoopt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sbUeyUYIbWNn","executionInfo":{"status":"ok","timestamp":1735124950272,"user_tz":-330,"elapsed":4115,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"a107e563-e14f-41f0-d70c-ee528b8b32fe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting geoopt\n","  Downloading geoopt-0.5.0-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from geoopt) (2.5.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.13.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->geoopt) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->geoopt) (3.0.2)\n","Downloading geoopt-0.5.0-py3-none-any.whl (90 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/90.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: geoopt\n","Successfully installed geoopt-0.5.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n","from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n","\n","# Base-LCM Architecture Components with Hyperbolic Space\n","class PreNet(nn.Module):\n","    \"\"\"\n","    Maps input embeddings to the model's hidden dimension in hyperbolic space.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, manifold):\n","        super(PreNet, self).__init__()\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","        self.manifold = manifold\n","        self.hidden_dim = hidden_dim\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.manifold.expmap0(x)  # Map to hyperbolic space (Poincare Ball)\n","        return x\n","\n","class PostNet(nn.Module):\n","    \"\"\"\n","    Maps hidden state outputs back to the embedding space from hyperbolic space.\n","    \"\"\"\n","    def __init__(self, hidden_dim, output_dim, manifold):\n","        super(PostNet, self).__init__()\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        x = self.manifold.logmap0(x)  # Map back to Euclidean space\n","        x = self.linear(x)\n","        return x\n","\n","class TransformerDecoder(nn.Module):\n","    \"\"\"\n","    Standard Decoder-Only Transformer operating in hyperbolic space.\n","    \"\"\"\n","    def __init__(self, hidden_dim, num_heads, num_layers, ff_dim, manifold, dropout=0.1):\n","        super(TransformerDecoder, self).__init__()\n","        self.layers = nn.ModuleList([\n","            nn.TransformerDecoderLayer(\n","                d_model=hidden_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout\n","            )\n","            for _ in range(num_layers)\n","        ])\n","        self.manifold = manifold\n","        self.pos_encoder = ManifoldParameter(torch.zeros(1, 512, hidden_dim), manifold=manifold)\n","\n","    def forward(self, x):\n","        seq_len = x.size(1)\n","        x = self.manifold.expmap0(x + self.pos_encoder[:,:seq_len])  # Ensure curvature is retained\n","        for layer in self.layers:\n","            x = layer(x, x)  # Self-attention in decoder layers\n","        return x\n","\n","class HyperbolicLCM(nn.Module):\n","    \"\"\"\n","    Base Large Concept Model (LCM) with Hyperbolic Hidden Space.\n","    - PreNet: Maps input embeddings to hyperbolic space.\n","    - TransformerDecoder: Operates in hyperbolic space.\n","    - PostNet: Maps back to Euclidean space.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim, manifold):\n","        super(HyperbolicLCM, self).__init__()\n","        self.manifold = manifold\n","        self.prenet = PreNet(input_dim, hidden_dim, manifold)\n","        self.transformer_decoder = TransformerDecoder(hidden_dim, num_heads, num_layers, ff_dim, manifold)\n","        self.postnet = PostNet(hidden_dim, output_dim, manifold)\n","\n","    def forward(self, x):\n","        x = self.prenet(x)\n","        x = self.transformer_decoder(x)\n","        x = self.postnet(x)\n","        return x\n","\n","# Cosine Similarity for Accuracy\n","def compute_accuracy(predicted, target, threshold=0.5):\n","    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n","    correct = (cos_sim > threshold).float()\n","    accuracy = correct.mean().item()\n","    return accuracy\n","\n","# Adding noise to target embeddings\n","def add_noise_to_embeddings(embeddings, noise_level=0.1):\n","    noise = torch.randn_like(embeddings) * noise_level\n","    return embeddings + noise\n","\n","# Testing the Hyperbolic-LCM Architecture\n","def test_hyperbolic_lcm():\n","    batch_size = 4\n","    sequence_length = 10\n","    input_dim = 256  # Input embedding dimension\n","    hidden_dim = 512  # Hidden dimension in hyperbolic space\n","    num_heads = 8\n","    num_layers = 6\n","    ff_dim = 2048\n","    output_dim = 256  # Output embedding dimension\n","    epochs = 5  # Number of epochs for training\n","    noise_level = 0.05  # Noise level for targets\n","\n","    # Initialize the Poincare Ball Manifold\n","    manifold = PoincareBall(c=1.0)  # Curvature = 1.0\n","\n","    # Random input to simulate embeddings\n","    input_embeddings = torch.randn(batch_size, sequence_length, input_dim)\n","\n","    # Initialize the Hyperbolic-LCM Model\n","    model = HyperbolicLCM(input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim, manifold)\n","\n","    # Define the Riemannian Adam optimizer\n","    optimizer = RiemannianAdam(model.parameters(), lr=1e-3)\n","    criterion = nn.MSELoss()\n","\n","    # Create Target Embeddings with Noise\n","    target_embeddings = add_noise_to_embeddings(input_embeddings, noise_level=noise_level)\n","\n","    # Training Loop for Multiple Epochs\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        output_embeddings = model(input_embeddings)\n","        loss = criterion(output_embeddings, target_embeddings)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute Accuracy\n","        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold=0.2)\n","\n","        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy * 100:.2f}%\")\n","\n","if __name__ == \"__main__\":\n","    test_hyperbolic_lcm()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qn83vhv_gYm_","executionInfo":{"status":"ok","timestamp":1735124981894,"user_tz":-330,"elapsed":7541,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"f1952316-f8ff-4f1e-f65e-91de82bcee67"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5 | Loss: 1.0427 | Accuracy: 0.00%\n","Epoch 2/5 | Loss: 0.9581 | Accuracy: 60.00%\n","Epoch 3/5 | Loss: 0.8878 | Accuracy: 100.00%\n","Epoch 4/5 | Loss: 0.8937 | Accuracy: 92.50%\n","Epoch 5/5 | Loss: 0.8720 | Accuracy: 100.00%\n"]}]},{"cell_type":"code","source":["!pip install torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0x38KvEgfef","executionInfo":{"status":"ok","timestamp":1735125001083,"user_tz":-330,"elapsed":3980,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"5beeb73b-2749-4e5f-8e12-7f591b82c47d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchtext\n","  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n","Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchtext\n","Successfully installed torchtext-0.18.0\n"]}]},{"cell_type":"code","source":["!pip uninstall torchtext --yes\n","!pip install torchtext --no-cache-dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLYrhluOglCo","executionInfo":{"status":"ok","timestamp":1735125020984,"user_tz":-330,"elapsed":4450,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"074ca256-016d-4257-cdc6-ac051bfd245e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torchtext 0.18.0\n","Uninstalling torchtext-0.18.0:\n","  Successfully uninstalled torchtext-0.18.0\n","Collecting torchtext\n","  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n","Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchtext\n","Successfully installed torchtext-0.18.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n","from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n","import re\n","\n","# -----------------------------\n","#  Pyramid and Hyperbolic Layers\n","# -----------------------------\n","class PyramidLayer(nn.Module):\n","    \"\"\"\n","    Represents one pyramid layer: compresses dimensionality in hyperbolic space.\n","    \"\"\"\n","    def __init__(self, input_dim, output_dim, manifold):\n","        super(PyramidLayer, self).__init__()\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        # Map to hyperbolic space with compression\n","        x = self.manifold.expmap0(self.linear(x))\n","        return x\n","\n","class HyperbolicCube(nn.Module):\n","    \"\"\"\n","    Hyperbolic Cube: Multiple pyramid layers forming a cube-like structure.\n","    \"\"\"\n","    def __init__(self, layers_dims, manifold):\n","        super(HyperbolicCube, self).__init__()\n","        self.manifold = manifold\n","        self.pyramid_layers = nn.ModuleList([\n","            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)\n","            for i in range(len(layers_dims) - 1)\n","        ])\n","\n","    def forward(self, x):\n","        for layer in self.pyramid_layers:\n","            x = layer(x)\n","        return x\n","\n","# -----------------------------\n","#  PreNet and PostNet\n","# -----------------------------\n","class PreNet(nn.Module):\n","    \"\"\"\n","    Maps input embeddings to the hidden dimension in hyperbolic space.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, manifold):\n","        super(PreNet, self).__init__()\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.manifold.expmap0(x)\n","        return x\n","\n","class PostNet(nn.Module):\n","    \"\"\"\n","    Maps output back to the embedding space from hyperbolic space.\n","    \"\"\"\n","    def __init__(self, hidden_dim, output_dim, manifold):\n","        super(PostNet, self).__init__()\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        x = self.manifold.logmap0(x)\n","        x = self.linear(x)\n","        return x\n","\n","# -----------------------------\n","#     Hyperbolic LCM Model\n","# -----------------------------\n","class HyperbolicLCM(nn.Module):\n","    \"\"\"\n","    LCM with a Hyperbolic Cube as the hidden space.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dims, num_heads, num_layers, ff_dim, output_dim, manifold):\n","        super(HyperbolicLCM, self).__init__()\n","        self.manifold = manifold\n","        self.prenet = PreNet(input_dim, hidden_dims[0], manifold)\n","        self.hyperbolic_cube = HyperbolicCube(hidden_dims, manifold)\n","        self.postnet = PostNet(hidden_dims[-1], output_dim, manifold)\n","\n","    def forward(self, x):\n","        x = self.prenet(x)\n","        x = self.hyperbolic_cube(x)\n","        x = self.postnet(x)\n","        return x\n","\n","# -----------------------------\n","#    Utility Functions\n","# -----------------------------\n","def compute_accuracy(predicted, target, threshold=0.1):\n","    \"\"\"\n","    Computes accuracy based on cosine similarity.\n","    \"\"\"\n","    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n","    correct = (cos_sim > threshold).float()\n","    accuracy = correct.mean().item()\n","    return accuracy\n","\n","def load_speech_and_create_embeddings(\n","    file_path,\n","    vocab_size=5000,\n","    embedding_dim=300,\n","    lowercase=True\n","):\n","    \"\"\"\n","    1. Read raw speech text from file.\n","    2. Tokenize into words (simple approach).\n","    3. Assign each unique word a random embedding of 'embedding_dim' size.\n","    4. Return dict: {word -> random_embedding}.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","\n","    # Optionally lowercase\n","    if lowercase:\n","        text = text.lower()\n","\n","    # Remove punctuation except apostrophes (minimal cleaning)\n","    # You can adapt this regex for your own punctuation rules\n","    text = re.sub(r'[^\\w\\s\\']', '', text)\n","\n","    # Split into tokens by whitespace\n","    tokens = text.split()\n","\n","    # Build a vocabulary of unique tokens, up to 'vocab_size'\n","    unique_tokens = list(dict.fromkeys(tokens))  # preserves order\n","    if len(unique_tokens) > vocab_size:\n","        unique_tokens = unique_tokens[:vocab_size]\n","\n","    # Assign random embeddings for each unique token\n","    # shape: (embedding_dim,), values ~ N(0, 1)\n","    embeddings_dict = {}\n","    for token in unique_tokens:\n","        embeddings_dict[token] = torch.randn(embedding_dim, dtype=torch.float)\n","\n","    return embeddings_dict\n","\n","def prepare_speech_batch(\n","    embeddings_dict,\n","    batch_size=4,\n","    sequence_length=10,\n","    embedding_dim=300\n","):\n","    \"\"\"\n","    Randomly choose 'sequence_length' words from your dict\n","    and build a batch of size (batch_size, sequence_length, embedding_dim).\n","    \"\"\"\n","    import random\n","    vocab_tokens = list(embeddings_dict.keys())\n","\n","    # If your dict doesn't have enough words, handle that:\n","    if len(vocab_tokens) < sequence_length:\n","        raise ValueError(\"Not enough unique words to form a sequence.\")\n","\n","    # Randomly pick 'sequence_length' words\n","    random_tokens = random.sample(vocab_tokens, sequence_length)\n","    # Stack embeddings\n","    selected_vectors = torch.stack([embeddings_dict[tok] for tok in random_tokens])\n","\n","    # Create a batch dimension: (batch_size, sequence_length, embedding_dim)\n","    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)\n","    return input_embeddings\n","\n","# -----------------------------\n","#   Testing Hyperbolic LCM\n","# -----------------------------\n","def test_hyperbolic_lcm():\n","    batch_size = 4\n","    sequence_length = 10\n","    input_dim = 300  # Embedding dimension\n","    hidden_dims = [512, 256, 128, 64]  # Pyramid structure dimensions\n","    output_dim = 300\n","    epochs = 70\n","    threshold = 0.1  # Cosine similarity threshold\n","    trump_file = \"trump_3.6.txt\"  # Path to Trump speech text\n","\n","    # 1. Read and tokenize speech, then assign random embeddings\n","    embeddings_dict = load_speech_and_create_embeddings(\n","        trump_file,\n","        vocab_size=2000,   # up to 2000 unique tokens\n","        embedding_dim=input_dim,\n","        lowercase=True\n","    )\n","\n","    # 2. Prepare a random batch from our speech \"vocab\"\n","    input_embeddings = prepare_speech_batch(\n","        embeddings_dict,\n","        batch_size=batch_size,\n","        sequence_length=sequence_length,\n","        embedding_dim=input_dim\n","    )\n","\n","    # Initialize the Poincare Ball Manifold\n","    manifold = PoincareBall(c=1.0)\n","\n","    # 3. Initialize Hyperbolic-LCM Model\n","    model = HyperbolicLCM(\n","        input_dim,\n","        hidden_dims,\n","        num_heads=8,\n","        num_layers=6,\n","        ff_dim=2048,\n","        output_dim=output_dim,\n","        manifold=manifold\n","    )\n","\n","    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)  # Riemannian-compatible optimizer\n","    criterion = nn.MSELoss()\n","\n","    # Create slightly perturbed target embeddings\n","    # shape: (batch_size, sequence_length, embedding_dim)\n","    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01\n","\n","    # 4. Training Loop\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        output_embeddings = model(input_embeddings)\n","        loss = criterion(output_embeddings, target_embeddings)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Compute Accuracy\n","        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)\n","        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy * 100:.2f}%\")\n","\n","if __name__ == \"__main__\":\n","    test_hyperbolic_lcm()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OECfax_xgpy4","executionInfo":{"status":"ok","timestamp":1735127755116,"user_tz":-330,"elapsed":1082,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"96ededc7-997c-4e40-8500-b32a5de11cc6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/70 | Loss: 0.9994 | Accuracy: 10.00%\n","Epoch 2/70 | Loss: 0.9986 | Accuracy: 10.00%\n","Epoch 3/70 | Loss: 0.9979 | Accuracy: 10.00%\n","Epoch 4/70 | Loss: 0.9972 | Accuracy: 10.00%\n","Epoch 5/70 | Loss: 0.9965 | Accuracy: 10.00%\n","Epoch 6/70 | Loss: 0.9957 | Accuracy: 10.00%\n","Epoch 7/70 | Loss: 0.9950 | Accuracy: 10.00%\n","Epoch 8/70 | Loss: 0.9943 | Accuracy: 10.00%\n","Epoch 9/70 | Loss: 0.9936 | Accuracy: 10.00%\n","Epoch 10/70 | Loss: 0.9929 | Accuracy: 15.00%\n","Epoch 11/70 | Loss: 0.9922 | Accuracy: 20.00%\n","Epoch 12/70 | Loss: 0.9915 | Accuracy: 20.00%\n","Epoch 13/70 | Loss: 0.9908 | Accuracy: 30.00%\n","Epoch 14/70 | Loss: 0.9901 | Accuracy: 30.00%\n","Epoch 15/70 | Loss: 0.9894 | Accuracy: 37.50%\n","Epoch 16/70 | Loss: 0.9887 | Accuracy: 40.00%\n","Epoch 17/70 | Loss: 0.9880 | Accuracy: 40.00%\n","Epoch 18/70 | Loss: 0.9873 | Accuracy: 40.00%\n","Epoch 19/70 | Loss: 0.9866 | Accuracy: 50.00%\n","Epoch 20/70 | Loss: 0.9859 | Accuracy: 50.00%\n","Epoch 21/70 | Loss: 0.9853 | Accuracy: 50.00%\n","Epoch 22/70 | Loss: 0.9846 | Accuracy: 50.00%\n","Epoch 23/70 | Loss: 0.9839 | Accuracy: 50.00%\n","Epoch 24/70 | Loss: 0.9832 | Accuracy: 50.00%\n","Epoch 25/70 | Loss: 0.9825 | Accuracy: 50.00%\n","Epoch 26/70 | Loss: 0.9818 | Accuracy: 50.00%\n","Epoch 27/70 | Loss: 0.9811 | Accuracy: 50.00%\n","Epoch 28/70 | Loss: 0.9804 | Accuracy: 57.50%\n","Epoch 29/70 | Loss: 0.9797 | Accuracy: 60.00%\n","Epoch 30/70 | Loss: 0.9790 | Accuracy: 60.00%\n","Epoch 31/70 | Loss: 0.9783 | Accuracy: 60.00%\n","Epoch 32/70 | Loss: 0.9775 | Accuracy: 70.00%\n","Epoch 33/70 | Loss: 0.9768 | Accuracy: 70.00%\n","Epoch 34/70 | Loss: 0.9761 | Accuracy: 80.00%\n","Epoch 35/70 | Loss: 0.9754 | Accuracy: 80.00%\n","Epoch 36/70 | Loss: 0.9746 | Accuracy: 80.00%\n","Epoch 37/70 | Loss: 0.9739 | Accuracy: 80.00%\n","Epoch 38/70 | Loss: 0.9732 | Accuracy: 80.00%\n","Epoch 39/70 | Loss: 0.9724 | Accuracy: 80.00%\n","Epoch 40/70 | Loss: 0.9717 | Accuracy: 80.00%\n","Epoch 41/70 | Loss: 0.9709 | Accuracy: 80.00%\n","Epoch 42/70 | Loss: 0.9702 | Accuracy: 80.00%\n","Epoch 43/70 | Loss: 0.9694 | Accuracy: 80.00%\n","Epoch 44/70 | Loss: 0.9686 | Accuracy: 80.00%\n","Epoch 45/70 | Loss: 0.9679 | Accuracy: 80.00%\n","Epoch 46/70 | Loss: 0.9671 | Accuracy: 80.00%\n","Epoch 47/70 | Loss: 0.9663 | Accuracy: 80.00%\n","Epoch 48/70 | Loss: 0.9655 | Accuracy: 80.00%\n","Epoch 49/70 | Loss: 0.9647 | Accuracy: 80.00%\n","Epoch 50/70 | Loss: 0.9639 | Accuracy: 80.00%\n","Epoch 51/70 | Loss: 0.9631 | Accuracy: 80.00%\n","Epoch 52/70 | Loss: 0.9623 | Accuracy: 80.00%\n","Epoch 53/70 | Loss: 0.9615 | Accuracy: 80.00%\n","Epoch 54/70 | Loss: 0.9606 | Accuracy: 92.50%\n","Epoch 55/70 | Loss: 0.9598 | Accuracy: 100.00%\n","Epoch 56/70 | Loss: 0.9590 | Accuracy: 100.00%\n","Epoch 57/70 | Loss: 0.9581 | Accuracy: 100.00%\n","Epoch 58/70 | Loss: 0.9573 | Accuracy: 100.00%\n","Epoch 59/70 | Loss: 0.9564 | Accuracy: 100.00%\n","Epoch 60/70 | Loss: 0.9556 | Accuracy: 100.00%\n","Epoch 61/70 | Loss: 0.9547 | Accuracy: 100.00%\n","Epoch 62/70 | Loss: 0.9539 | Accuracy: 100.00%\n","Epoch 63/70 | Loss: 0.9530 | Accuracy: 100.00%\n","Epoch 64/70 | Loss: 0.9521 | Accuracy: 100.00%\n","Epoch 65/70 | Loss: 0.9512 | Accuracy: 100.00%\n","Epoch 66/70 | Loss: 0.9503 | Accuracy: 100.00%\n","Epoch 67/70 | Loss: 0.9494 | Accuracy: 100.00%\n","Epoch 68/70 | Loss: 0.9485 | Accuracy: 100.00%\n","Epoch 69/70 | Loss: 0.9476 | Accuracy: 100.00%\n","Epoch 70/70 | Loss: 0.9467 | Accuracy: 100.00%\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import re\n","import random\n","\n","from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n","from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n","\n","\n","# -----------------------------\n","#  Pyramid and Hyperbolic Layers\n","# -----------------------------\n","class PyramidLayer(nn.Module):\n","    \"\"\"\n","    Represents one pyramid layer: compresses dimensionality in hyperbolic space.\n","    \"\"\"\n","    def __init__(self, input_dim, output_dim, manifold):\n","        super(PyramidLayer, self).__init__()\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        # Map to hyperbolic space with compression\n","        x = self.manifold.expmap0(self.linear(x))\n","        return x\n","\n","\n","class HyperbolicCube(nn.Module):\n","    \"\"\"\n","    Hyperbolic Cube: Multiple pyramid layers forming a cube-like structure.\n","    \"\"\"\n","    def __init__(self, layers_dims, manifold):\n","        super(HyperbolicCube, self).__init__()\n","        self.manifold = manifold\n","        self.pyramid_layers = nn.ModuleList([\n","            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)\n","            for i in range(len(layers_dims) - 1)\n","        ])\n","\n","    def forward(self, x):\n","        for layer in self.pyramid_layers:\n","            x = layer(x)\n","        return x\n","\n","\n","# -----------------------------\n","#  PreNet and PostNet\n","# -----------------------------\n","class PreNet(nn.Module):\n","    \"\"\"\n","    Maps input embeddings to the hidden dimension.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, manifold):\n","        super(PreNet, self).__init__()\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.manifold.expmap0(x)\n","        return x\n","\n","\n","class PostNet(nn.Module):\n","    \"\"\"\n","    Maps output back to the embedding space.\n","    \"\"\"\n","    def __init__(self, hidden_dim, output_dim, manifold):\n","        super(PostNet, self).__init__()\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        x = self.manifold.logmap0(x)\n","        x = self.linear(x)\n","        return x\n","\n","\n","# -----------------------------\n","#   Hyperbolic LCM Model\n","# -----------------------------\n","class HyperbolicLCM(nn.Module):\n","    \"\"\"\n","    LCM with a Hyperbolic Cube as the hidden space and a learnable curvature.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dims, num_heads, num_layers, ff_dim, output_dim):\n","        super(HyperbolicLCM, self).__init__()\n","        # Learnable curvature parameter, initialized at 1.0\n","        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n","        self.manifold = PoincareBall(c=self.curvature)\n","\n","        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)\n","        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)\n","        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)\n","\n","    def forward(self, x):\n","        x = self.prenet(x)\n","        x = self.hyperbolic_cube(x)\n","        x = self.postnet(x)\n","        return x\n","\n","\n","# -----------------------------\n","#  Utility Functions\n","# -----------------------------\n","def compute_accuracy(predicted, target, threshold=0.1):\n","    \"\"\"\n","    Computes accuracy based on cosine similarity.\n","    \"\"\"\n","    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n","    correct = (cos_sim > threshold).float()\n","    accuracy = correct.mean().item()\n","    return accuracy\n","\n","\n","def load_text_and_create_embeddings(\n","    file_path,\n","    embedding_dim=300,\n","    vocab_size=5000,\n","    lowercase=True\n","):\n","    \"\"\"\n","    1. Read raw text from file_path.\n","    2. Tokenize (simplistic).\n","    3. Build a vocabulary of unique tokens (up to vocab_size).\n","    4. Assign a random embedding of 'embedding_dim' for each token.\n","    Returns a dict: {token: embedding_tensor}.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","\n","    # Basic cleaning/lowercasing\n","    if lowercase:\n","        text = text.lower()\n","\n","    # Remove punctuation except apostrophes\n","    text = re.sub(r'[^\\w\\s\\']', '', text)\n","\n","    # Split by whitespace into tokens\n","    tokens = text.split()\n","\n","    # Build a vocabulary of unique tokens\n","    unique_tokens = list(dict.fromkeys(tokens))  # preserves order of first occurrence\n","    if len(unique_tokens) > vocab_size:\n","        unique_tokens = unique_tokens[:vocab_size]\n","\n","    # Assign random embeddings\n","    embeddings_dict = {}\n","    for token in unique_tokens:\n","        embeddings_dict[token] = torch.randn(embedding_dim)\n","\n","    return embeddings_dict\n","\n","\n","def prepare_input_batch(embeddings_dict, batch_size=4, sequence_length=10):\n","    \"\"\"\n","    Randomly picks 'sequence_length' distinct words from 'embeddings_dict',\n","    builds a (batch_size, sequence_length, embedding_dim) tensor.\n","    \"\"\"\n","    vocab_tokens = list(embeddings_dict.keys())\n","    embedding_dim = len(next(iter(embeddings_dict.values())))  # dimension of the 1st embedding\n","\n","    if len(vocab_tokens) < sequence_length:\n","        raise ValueError(\"Not enough unique tokens to form the sequence.\")\n","\n","    # Randomly select tokens\n","    random_tokens = random.sample(vocab_tokens, sequence_length)\n","    selected_vectors = torch.stack([embeddings_dict[tok] for tok in random_tokens])\n","\n","    # Expand to create batch: (batch_size, sequence_length, embedding_dim)\n","    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)\n","    return input_embeddings\n","\n","\n","# -----------------------------\n","#  Testing Hyperbolic-LCM\n","# -----------------------------\n","def test_hyperbolic_lcm():\n","    batch_size = 4\n","    sequence_length = 10\n","    input_dim = 300  # embedding dimension\n","    hidden_dims = [512, 256, 128, 64]\n","    output_dim = 300\n","\n","    epochs = 40\n","    threshold = 0.1\n","    text_file = \"trump_3.6.txt\"  # or any raw text file\n","\n","    # 1) Load text and create random embeddings for each word\n","    embeddings_dict = load_text_and_create_embeddings(\n","        text_file,\n","        embedding_dim=input_dim,\n","        vocab_size=2000,    # up to 2000 unique words\n","        lowercase=True\n","    )\n","\n","    # 2) Prepare input batch\n","    input_embeddings = prepare_input_batch(\n","        embeddings_dict,\n","        batch_size=batch_size,\n","        sequence_length=sequence_length\n","    )\n","\n","    # 3) Initialize the Hyperbolic-LCM model with learnable curvature\n","    model = HyperbolicLCM(input_dim, hidden_dims, num_heads=8, num_layers=6, ff_dim=2048, output_dim=output_dim)\n","    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)\n","    criterion = nn.MSELoss()\n","\n","    # Create target embeddings by adding minimal noise\n","    target_embeddings = input_embeddings + 0.01 * torch.randn_like(input_embeddings)\n","\n","    # 4) Training Loop\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        output_embeddings = model(input_embeddings)\n","\n","        # Main loss\n","        loss = criterion(output_embeddings, target_embeddings)\n","\n","        # Curvature regularization:\n","        # Encourage curvature to stay near 1.0, but allow it to drift slightly.\n","        curvature_reg = torch.abs(model.curvature - 1.0) * 0.01\n","        total_loss = loss + curvature_reg\n","\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        # Compute accuracy\n","        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)\n","\n","        print(\n","            f\"Epoch {epoch + 1}/{epochs} | \"\n","            f\"Loss: {loss.item():.4f} | \"\n","            f\"Curvature: {model.curvature.item():.4f} | \"\n","            f\"Accuracy: {accuracy * 100:.2f}%\"\n","        )\n","\n","\n","if __name__ == \"__main__\":\n","    test_hyperbolic_lcm()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cPhWClp8gxJC","executionInfo":{"status":"ok","timestamp":1735127924576,"user_tz":-330,"elapsed":1379,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"30cfe980-00b7-40b3-e4af-2646efd5aed0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40 | Loss: 0.9819 | Curvature: 0.5414 | Accuracy: 0.00%\n","Epoch 2/40 | Loss: 0.9812 | Curvature: 0.5415 | Accuracy: 0.00%\n","Epoch 3/40 | Loss: 0.9805 | Curvature: 0.5416 | Accuracy: 0.00%\n","Epoch 4/40 | Loss: 0.9799 | Curvature: 0.5417 | Accuracy: 0.00%\n","Epoch 5/40 | Loss: 0.9792 | Curvature: 0.5418 | Accuracy: 0.00%\n","Epoch 6/40 | Loss: 0.9786 | Curvature: 0.5419 | Accuracy: 0.00%\n","Epoch 7/40 | Loss: 0.9779 | Curvature: 0.5420 | Accuracy: 0.00%\n","Epoch 8/40 | Loss: 0.9773 | Curvature: 0.5421 | Accuracy: 0.00%\n","Epoch 9/40 | Loss: 0.9766 | Curvature: 0.5422 | Accuracy: 10.00%\n","Epoch 10/40 | Loss: 0.9760 | Curvature: 0.5423 | Accuracy: 20.00%\n","Epoch 11/40 | Loss: 0.9754 | Curvature: 0.5424 | Accuracy: 20.00%\n","Epoch 12/40 | Loss: 0.9747 | Curvature: 0.5425 | Accuracy: 20.00%\n","Epoch 13/40 | Loss: 0.9741 | Curvature: 0.5426 | Accuracy: 20.00%\n","Epoch 14/40 | Loss: 0.9735 | Curvature: 0.5427 | Accuracy: 20.00%\n","Epoch 15/40 | Loss: 0.9728 | Curvature: 0.5428 | Accuracy: 20.00%\n","Epoch 16/40 | Loss: 0.9722 | Curvature: 0.5429 | Accuracy: 20.00%\n","Epoch 17/40 | Loss: 0.9716 | Curvature: 0.5430 | Accuracy: 20.00%\n","Epoch 18/40 | Loss: 0.9709 | Curvature: 0.5431 | Accuracy: 30.00%\n","Epoch 19/40 | Loss: 0.9703 | Curvature: 0.5432 | Accuracy: 30.00%\n","Epoch 20/40 | Loss: 0.9697 | Curvature: 0.5433 | Accuracy: 30.00%\n","Epoch 21/40 | Loss: 0.9690 | Curvature: 0.5434 | Accuracy: 50.00%\n","Epoch 22/40 | Loss: 0.9684 | Curvature: 0.5435 | Accuracy: 50.00%\n","Epoch 23/40 | Loss: 0.9678 | Curvature: 0.5436 | Accuracy: 50.00%\n","Epoch 24/40 | Loss: 0.9671 | Curvature: 0.5437 | Accuracy: 50.00%\n","Epoch 25/40 | Loss: 0.9665 | Curvature: 0.5438 | Accuracy: 52.50%\n","Epoch 26/40 | Loss: 0.9658 | Curvature: 0.5439 | Accuracy: 60.00%\n","Epoch 27/40 | Loss: 0.9652 | Curvature: 0.5440 | Accuracy: 60.00%\n","Epoch 28/40 | Loss: 0.9645 | Curvature: 0.5441 | Accuracy: 60.00%\n","Epoch 29/40 | Loss: 0.9639 | Curvature: 0.5442 | Accuracy: 60.00%\n","Epoch 30/40 | Loss: 0.9632 | Curvature: 0.5443 | Accuracy: 60.00%\n","Epoch 31/40 | Loss: 0.9626 | Curvature: 0.5444 | Accuracy: 60.00%\n","Epoch 32/40 | Loss: 0.9619 | Curvature: 0.5445 | Accuracy: 60.00%\n","Epoch 33/40 | Loss: 0.9612 | Curvature: 0.5446 | Accuracy: 70.00%\n","Epoch 34/40 | Loss: 0.9606 | Curvature: 0.5447 | Accuracy: 70.00%\n","Epoch 35/40 | Loss: 0.9599 | Curvature: 0.5448 | Accuracy: 70.00%\n","Epoch 36/40 | Loss: 0.9592 | Curvature: 0.5449 | Accuracy: 70.00%\n","Epoch 37/40 | Loss: 0.9585 | Curvature: 0.5450 | Accuracy: 70.00%\n","Epoch 38/40 | Loss: 0.9578 | Curvature: 0.5451 | Accuracy: 70.00%\n","Epoch 39/40 | Loss: 0.9571 | Curvature: 0.5452 | Accuracy: 70.00%\n","Epoch 40/40 | Loss: 0.9564 | Curvature: 0.5453 | Accuracy: 70.00%\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import re\n","import random\n","\n","from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n","from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n","\n","# --------------------------------------------------\n","# Utility: Compute Accuracy via Cosine Similarity\n","# --------------------------------------------------\n","def compute_accuracy(predicted, target, threshold=0.1):\n","    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n","    correct = (cos_sim > threshold).float()\n","    accuracy = correct.mean().item()\n","    return accuracy\n","\n","\n","# --------------------------------------------------\n","# Utility: Load Raw Text, Create Random Embeddings\n","# --------------------------------------------------\n","def load_text_and_create_embeddings(\n","    file_path,\n","    embedding_dim=300,\n","    vocab_size=5000,\n","    lowercase=True\n","):\n","    \"\"\"\n","    1. Reads raw text from file_path.\n","    2. Basic tokenization: remove punctuation (except apostrophes) + split on whitespace.\n","    3. Builds up to 'vocab_size' unique tokens.\n","    4. Assigns each token a random vector of size 'embedding_dim'.\n","    Returns {token -> embedding_vector}.\n","    \"\"\"\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        text = f.read()\n","\n","    # Optional lowercasing\n","    if lowercase:\n","        text = text.lower()\n","\n","    # Remove punctuation except apostrophes\n","    text = re.sub(r\"[^\\w\\s\\']\", \"\", text)\n","\n","    # Split on whitespace\n","    tokens = text.split()\n","\n","    # Grab unique tokens in order of appearance\n","    unique_tokens = list(dict.fromkeys(tokens))\n","    if len(unique_tokens) > vocab_size:\n","        unique_tokens = unique_tokens[:vocab_size]\n","\n","    # Assign random embeddings\n","    embeddings_dict = {}\n","    for token in unique_tokens:\n","        embeddings_dict[token] = torch.randn(embedding_dim, dtype=torch.float)\n","\n","    return embeddings_dict\n","\n","\n","def prepare_input_batch(\n","    embeddings_dict,\n","    batch_size=4,\n","    sequence_length=10\n","):\n","    \"\"\"\n","    Randomly picks 'sequence_length' tokens from embeddings_dict, stacks them,\n","    and repeats them 'batch_size' times.\n","    Returns a (batch_size, sequence_length, embedding_dim) tensor.\n","    \"\"\"\n","    vocab_tokens = list(embeddings_dict.keys())\n","    if len(vocab_tokens) < sequence_length:\n","        raise ValueError(\"Not enough unique tokens to form a sequence.\")\n","\n","    # Embedding dimension from the first token\n","    embedding_dim = embeddings_dict[vocab_tokens[0]].shape[0]\n","\n","    # Randomly pick 'sequence_length' tokens\n","    random_tokens = random.sample(vocab_tokens, sequence_length)\n","    selected_vectors = torch.stack([embeddings_dict[tok] for tok in random_tokens])\n","\n","    # Expand to (batch_size, sequence_length, embedding_dim)\n","    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)\n","    return input_embeddings\n","\n","\n","# --------------------------------------------------\n","# Pyramid + Hyperbolic Cube\n","# --------------------------------------------------\n","class PyramidLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim, manifold):\n","        super(PyramidLayer, self).__init__()\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        # Hyperbolic compression\n","        x = self.manifold.expmap0(self.linear(x))\n","        return x\n","\n","\n","class HyperbolicCube(nn.Module):\n","    def __init__(self, layers_dims, manifold):\n","        super(HyperbolicCube, self).__init__()\n","        self.manifold = manifold\n","        self.pyramid_layers = nn.ModuleList([\n","            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)\n","            for i in range(len(layers_dims) - 1)\n","        ])\n","\n","    def forward(self, x):\n","        for layer in self.pyramid_layers:\n","            x = layer(x)\n","        return x\n","\n","\n","# --------------------------------------------------\n","# PreNet and PostNet\n","# --------------------------------------------------\n","class PreNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, manifold):\n","        super(PreNet, self).__init__()\n","        self.linear = nn.Linear(input_dim, hidden_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.manifold.expmap0(x)\n","        return x\n","\n","\n","class PostNet(nn.Module):\n","    def __init__(self, hidden_dim, output_dim, manifold):\n","        super(PostNet, self).__init__()\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","        self.manifold = manifold\n","\n","    def forward(self, x):\n","        x = self.manifold.logmap0(x)\n","        x = self.linear(x)\n","        return x\n","\n","\n","# --------------------------------------------------\n","# Dual Hidden LCM\n","# --------------------------------------------------\n","class DualHiddenLCM(nn.Module):\n","    \"\"\"\n","    1) A hyperbolic path (PreNet -> HyperbolicCube -> PostNet)\n","    2) A simple 20D feedforward bottleneck\n","    Outputs are combined (summed).\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dims, hidden_dim2, output_dim):\n","        super(DualHiddenLCM, self).__init__()\n","        # Learnable curvature for PoincareBall\n","        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n","        self.manifold = PoincareBall(c=self.curvature)\n","\n","        # Hidden Dim Path 1\n","        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)\n","        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)\n","        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)\n","\n","        # Hidden Dim Path 2: simple feedforward\n","        self.hidden_dim2 = nn.Linear(input_dim, hidden_dim2)\n","        self.hidden_dim2_output = nn.Linear(hidden_dim2, output_dim)\n","\n","    def forward(self, x):\n","        # Path 1: Hyperbolic\n","        x_hidden1 = self.prenet(x)\n","        x_hidden1 = self.hyperbolic_cube(x_hidden1)\n","        x_hidden1 = self.postnet(x_hidden1)\n","\n","        # Path 2: Standard feedforward\n","        x_hidden2 = F.relu(self.hidden_dim2(x))  # 20D bottleneck\n","        x_hidden2 = self.hidden_dim2_output(x_hidden2)\n","\n","        # Combine outputs\n","        combined = x_hidden1 + x_hidden2\n","        return combined\n","\n","\n","# --------------------------------------------------\n","# Test DualHiddenLCM\n","# --------------------------------------------------\n","def test_dualhidden_lcm():\n","    batch_size = 4\n","    sequence_length = 10\n","    input_dim = 300  # Word embedding dimension\n","    hidden_dims = [512, 256, 128, 64]\n","    hidden_dim2 = 20\n","    output_dim = 300\n","\n","    epochs = 60\n","    threshold = 0.1  # Cosine similarity threshold\n","\n","    # Path to your raw text (e.g., trump_3.6.txt)\n","    text_file = \"trump_3.6.txt\"\n","\n","    # 1) Create random embeddings from the text\n","    embeddings_dict = load_text_and_create_embeddings(\n","        file_path=text_file,\n","        embedding_dim=input_dim,\n","        vocab_size=2000,\n","        lowercase=True\n","    )\n","\n","    # 2) Prepare input batch\n","    input_embeddings = prepare_input_batch(\n","        embeddings_dict,\n","        batch_size=batch_size,\n","        sequence_length=sequence_length\n","    )\n","\n","    # 3) Initialize Model\n","    model = DualHiddenLCM(input_dim, hidden_dims, hidden_dim2, output_dim)\n","    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)\n","    criterion = nn.MSELoss()\n","\n","    # Create slightly perturbed target embeddings\n","    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01\n","\n","    # 4) Training Loop\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        output_embeddings = model(input_embeddings)\n","        loss = criterion(output_embeddings, target_embeddings)\n","\n","        # Curvature regularization (keep it near 1.0):\n","        curvature_reg = torch.abs(model.curvature - 1.0) * 0.01\n","        total_loss = loss + curvature_reg\n","\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        # Compute Accuracy\n","        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)\n","\n","        print(\n","            f\"Epoch {epoch + 1}/{epochs} | \"\n","            f\"Loss: {loss.item():.4f} | \"\n","            f\"Curvature: {model.curvature.item():.4f} | \"\n","            f\"Accuracy: {accuracy * 100:.2f}%\"\n","        )\n","\n","\n","if __name__ == \"__main__\":\n","    test_dualhidden_lcm()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLAuRYggrvaZ","executionInfo":{"status":"ok","timestamp":1735128136820,"user_tz":-330,"elapsed":1159,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"329f1d5f-7530-48b5-ac7b-ffa706dd443f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/60 | Loss: 1.0019 | Curvature: 0.5414 | Accuracy: 0.00%\n","Epoch 2/60 | Loss: 0.9986 | Curvature: 0.5415 | Accuracy: 0.00%\n","Epoch 3/60 | Loss: 0.9954 | Curvature: 0.5416 | Accuracy: 0.00%\n","Epoch 4/60 | Loss: 0.9923 | Curvature: 0.5417 | Accuracy: 0.00%\n","Epoch 5/60 | Loss: 0.9893 | Curvature: 0.5418 | Accuracy: 0.00%\n","Epoch 6/60 | Loss: 0.9864 | Curvature: 0.5419 | Accuracy: 0.00%\n","Epoch 7/60 | Loss: 0.9835 | Curvature: 0.5420 | Accuracy: 0.00%\n","Epoch 8/60 | Loss: 0.9807 | Curvature: 0.5421 | Accuracy: 0.00%\n","Epoch 9/60 | Loss: 0.9780 | Curvature: 0.5422 | Accuracy: 10.00%\n","Epoch 10/60 | Loss: 0.9753 | Curvature: 0.5423 | Accuracy: 20.00%\n","Epoch 11/60 | Loss: 0.9727 | Curvature: 0.5424 | Accuracy: 20.00%\n","Epoch 12/60 | Loss: 0.9701 | Curvature: 0.5425 | Accuracy: 22.50%\n","Epoch 13/60 | Loss: 0.9676 | Curvature: 0.5426 | Accuracy: 30.00%\n","Epoch 14/60 | Loss: 0.9652 | Curvature: 0.5427 | Accuracy: 30.00%\n","Epoch 15/60 | Loss: 0.9628 | Curvature: 0.5428 | Accuracy: 40.00%\n","Epoch 16/60 | Loss: 0.9605 | Curvature: 0.5429 | Accuracy: 40.00%\n","Epoch 17/60 | Loss: 0.9583 | Curvature: 0.5430 | Accuracy: 50.00%\n","Epoch 18/60 | Loss: 0.9561 | Curvature: 0.5431 | Accuracy: 50.00%\n","Epoch 19/60 | Loss: 0.9540 | Curvature: 0.5432 | Accuracy: 50.00%\n","Epoch 20/60 | Loss: 0.9520 | Curvature: 0.5433 | Accuracy: 50.00%\n","Epoch 21/60 | Loss: 0.9500 | Curvature: 0.5434 | Accuracy: 55.00%\n","Epoch 22/60 | Loss: 0.9480 | Curvature: 0.5435 | Accuracy: 60.00%\n","Epoch 23/60 | Loss: 0.9461 | Curvature: 0.5436 | Accuracy: 60.00%\n","Epoch 24/60 | Loss: 0.9443 | Curvature: 0.5437 | Accuracy: 60.00%\n","Epoch 25/60 | Loss: 0.9424 | Curvature: 0.5438 | Accuracy: 60.00%\n","Epoch 26/60 | Loss: 0.9406 | Curvature: 0.5439 | Accuracy: 70.00%\n","Epoch 27/60 | Loss: 0.9388 | Curvature: 0.5440 | Accuracy: 80.00%\n","Epoch 28/60 | Loss: 0.9371 | Curvature: 0.5441 | Accuracy: 80.00%\n","Epoch 29/60 | Loss: 0.9354 | Curvature: 0.5442 | Accuracy: 80.00%\n","Epoch 30/60 | Loss: 0.9337 | Curvature: 0.5443 | Accuracy: 80.00%\n","Epoch 31/60 | Loss: 0.9320 | Curvature: 0.5444 | Accuracy: 80.00%\n","Epoch 32/60 | Loss: 0.9304 | Curvature: 0.5445 | Accuracy: 80.00%\n","Epoch 33/60 | Loss: 0.9287 | Curvature: 0.5446 | Accuracy: 80.00%\n","Epoch 34/60 | Loss: 0.9272 | Curvature: 0.5447 | Accuracy: 80.00%\n","Epoch 35/60 | Loss: 0.9257 | Curvature: 0.5448 | Accuracy: 80.00%\n","Epoch 36/60 | Loss: 0.9241 | Curvature: 0.5449 | Accuracy: 80.00%\n","Epoch 37/60 | Loss: 0.9227 | Curvature: 0.5450 | Accuracy: 80.00%\n","Epoch 38/60 | Loss: 0.9212 | Curvature: 0.5451 | Accuracy: 80.00%\n","Epoch 39/60 | Loss: 0.9198 | Curvature: 0.5452 | Accuracy: 85.00%\n","Epoch 40/60 | Loss: 0.9184 | Curvature: 0.5453 | Accuracy: 100.00%\n","Epoch 41/60 | Loss: 0.9170 | Curvature: 0.5454 | Accuracy: 100.00%\n","Epoch 42/60 | Loss: 0.9156 | Curvature: 0.5455 | Accuracy: 100.00%\n","Epoch 43/60 | Loss: 0.9142 | Curvature: 0.5456 | Accuracy: 100.00%\n","Epoch 44/60 | Loss: 0.9128 | Curvature: 0.5457 | Accuracy: 100.00%\n","Epoch 45/60 | Loss: 0.9114 | Curvature: 0.5458 | Accuracy: 100.00%\n","Epoch 46/60 | Loss: 0.9101 | Curvature: 0.5459 | Accuracy: 100.00%\n","Epoch 47/60 | Loss: 0.9087 | Curvature: 0.5460 | Accuracy: 100.00%\n","Epoch 48/60 | Loss: 0.9074 | Curvature: 0.5461 | Accuracy: 100.00%\n","Epoch 49/60 | Loss: 0.9061 | Curvature: 0.5462 | Accuracy: 100.00%\n","Epoch 50/60 | Loss: 0.9047 | Curvature: 0.5463 | Accuracy: 100.00%\n","Epoch 51/60 | Loss: 0.9034 | Curvature: 0.5464 | Accuracy: 100.00%\n","Epoch 52/60 | Loss: 0.9021 | Curvature: 0.5465 | Accuracy: 100.00%\n","Epoch 53/60 | Loss: 0.9008 | Curvature: 0.5466 | Accuracy: 100.00%\n","Epoch 54/60 | Loss: 0.8995 | Curvature: 0.5467 | Accuracy: 100.00%\n","Epoch 55/60 | Loss: 0.8982 | Curvature: 0.5468 | Accuracy: 100.00%\n","Epoch 56/60 | Loss: 0.8968 | Curvature: 0.5469 | Accuracy: 100.00%\n","Epoch 57/60 | Loss: 0.8955 | Curvature: 0.5470 | Accuracy: 100.00%\n","Epoch 58/60 | Loss: 0.8942 | Curvature: 0.5471 | Accuracy: 100.00%\n","Epoch 59/60 | Loss: 0.8928 | Curvature: 0.5472 | Accuracy: 100.00%\n","Epoch 60/60 | Loss: 0.8915 | Curvature: 0.5473 | Accuracy: 100.00%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yOSqS0yhsjRu"},"execution_count":null,"outputs":[]}]}